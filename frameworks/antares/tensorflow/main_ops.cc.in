// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#include "tensorflow/core/framework/common_shape_fns.h"
#include "tensorflow/core/framework/op.h"
#include "tensorflow/core/framework/op_kernel.h"

#include "tensorflow/core/framework/tensor.h"
#include "tensorflow/core/platform/mutex.h"
#include "tensorflow/core/platform/stream_executor.h"
#include "tensorflow/core/platform/env.h"
#include "tensorflow/core/lib/io/path.h"

#include <vector>

#if GOOGLE_CUDA
#ifndef __HIP_PLATFORM_HCC__
#include <cuda.h>
#include <cuda_runtime_api.h>
#else
#include <hip/hip_runtime_api.h>

#define CUmodule hipModule_t
#define CUfunction hipFunction_t

#define cuModuleLoad hipModuleLoad
#define cuModuleUnload hipModuleUnload
#define cuModuleGetFunction hipModuleGetFunction

#define cuLaunchKernel(f, bx, by, bz, tx, ty, tz, shm, stream, args, extra) \
        hipModuleLaunchKernel(f, bx, by, bz, tx, ty, tz, shm, stream, args, extra)

#define cudaSuccess hipSuccess
#define cudaSetDevice hipSetDevice
#define cudaMallocHost hipHostMalloc
#define cudaFreeHost hipHostFree
#define cudaStream_t hipStream_t
#define cudaMemcpyAsync hipMemcpyAsync
#define cudaMemcpyHostToDevice hipMemcpyHostToDevice
#define cudaStreamSynchronize hipStreamSynchronize
#define cudaEvent_t hipEvent_t
#define cudaEventCreateWithFlags hipEventCreateWithFlags
#define cudaEventRecord hipEventRecord
#define cudaEventQuery hipEventQuery
#define cudaEventDestroy hipEventDestroy
#define cudaErrorNotReady hipErrorNotReady
#define cudaEventDisableTiming 0

#endif
#endif


namespace tensorflow {
namespace {

using namespace std;

typedef Eigen::ThreadPoolDevice CPUDevice;
typedef Eigen::GpuDevice GPUDevice;


template <typename Device>
class MainOpKernel: public AsyncOpKernel {
 public:

  explicit MainOpKernel(OpKernelConstruction* c)
      : AsyncOpKernel(c) {
    OP_REQUIRES_OK(c, c->GetAttr("source", &source));
    OP_REQUIRES_OK(c, c->GetAttr("kernel_path", &kernel_path));
    OP_REQUIRES_OK(c, c->GetAttr("antares_ir", &antares_ir));
    OP_REQUIRES_OK(c, c->GetAttr("meta_inputs", &meta_inputs));
    OP_REQUIRES_OK(c, c->GetAttr("meta_outputs", &meta_outputs));

    LOG(INFO) << "MainOpKernel(num_in=" << meta_inputs.size() << ", num_out=" << meta_outputs.size() << ", ir=`" << antares_ir << "`..)";
    LOG(INFO) << "MainOpKernel is compiling the dynamtic kernel..";

    CHECK_EQ(cuModuleLoad(&hmod, kernel_path.c_str()), 0);
    CHECK_EQ(cuModuleGetFunction(&hfunc, hmod, "template_op_kernel0"), 0);

    int i, pos, next;
    pos = source.find("// [thread_extent] blockIdx.x"), next = source.find("= ", pos + 1), bx = (pos >= 0 && next >= 0) ? std::atoi(source.c_str() + next + 2) : 1;
    pos = source.find("// [thread_extent] blockIdx.y"), next = source.find("= ", pos + 1), by = (pos >= 0 && next >= 0) ? std::atoi(source.c_str() + next + 2) : 1;
    pos = source.find("// [thread_extent] blockIdx.z"), next = source.find("= ", pos + 1), bz = (pos >= 0 && next >= 0) ? std::atoi(source.c_str() + next + 2) : 1;
    pos = source.find("// [thread_extent] threadIdx.x"), next = source.find("= ", pos + 1), tx = (pos >= 0 && next >= 0) ? std::atoi(source.c_str() + next + 2) : 1;
    pos = source.find("// [thread_extent] threadIdx.y"), next = source.find("= ", pos + 1), ty = (pos >= 0 && next >= 0) ? std::atoi(source.c_str() + next + 2) : 1;
    pos = source.find("// [thread_extent] threadIdx.z"), next = source.find("= ", pos + 1), tz = (pos >= 0 && next >= 0) ? std::atoi(source.c_str() + next + 2) : 1;

    pos = source.find(") {\n"), next = source.rfind('(', pos) + 1;
    CHECK_EQ(true, (pos > 0 && next > 0));
    auto code_args = source.substr(next, pos - next) + ",";
    args.resize(meta_inputs.size() + meta_outputs.size()), p_args.resize(args.size());
    for (i = pos = 0; next = code_args.find(',', pos), next >= 0; pos = next + 1, ++i) {
      int at = code_args.rfind(' ', next) + 1;
      auto arg_name = code_args.substr(at, next - at);
      CHECK_NE(arg_name, "");
      if (arg_name[0] == 'i')
        p_args[i] = &args[std::atoi(arg_name.c_str() + 5)];
      else
        p_args[i] = &args[meta_inputs.size() + std::atoi(arg_name.c_str() + 6)];
    }

    output_shapes.clear();
    for (int y = 0; y < meta_outputs.size(); ++y) {
      auto meta_shape = meta_outputs[y].substr(0, meta_outputs[y].find('/')) + "-";
      std::vector<int64> shape_builder;
      for (int i = 0, j = 1; j < meta_shape.size(); ++j) {
        if (meta_shape[j] == '-')
          shape_builder.push_back(std::atoi(meta_shape.c_str() + i)), i = j + 1;
      }
      output_shapes.push_back(std::move(shape_builder));
    }
  }

  ~MainOpKernel() {
    LOG(INFO) << "~MainOpKernel(..)";
    if (hmod != nullptr) {
      LOG(INFO) << "MainOpKernel is releasing the dynamtic kernel..";
      CHECK_EQ(cuModuleUnload(hmod), 0);
      hfunc = nullptr;
    }
  }

  void ComputeAsync(OpKernelContext* c, DoneCallback done) override {
    // LOG(INFO) << "ComputeAsync(..)";

    cudaStream_t cu_stream = *reinterpret_cast<const cudaStream_t*>(c->op_device_context()->stream()->implementation()->GpuStreamMemberHack());

    std::vector<Tensor*> outputs(meta_outputs.size());
    for (int i = 0; i < outputs.size(); ++i) {
      OP_REQUIRES_OK_ASYNC(c, c->allocate_output(i, tensorflow::TensorShape(gtl::ArraySlice<int64>(output_shapes[i].data(), output_shapes[i].size())), &outputs[i]), done);
    }

    for (int i = 0; i < meta_inputs.size(); ++i)
      args[i] = (void*)c->input(i).tensor_data().data();
    for (int i = 0; i < meta_outputs.size(); ++i)
      args[meta_inputs.size() + i] = (void*)outputs[i]->tensor_data().data();

    CHECK_EQ(cuLaunchKernel(hfunc, bx, by, bz, tx, ty, tz, 0, cu_stream, p_args.data(), NULL), 0);
    done();
  }

 private:
  CUmodule hmod = nullptr;
  CUfunction hfunc = nullptr;

  std::string source, antares_ir, kernel_path;
  std::vector<std::string> meta_inputs, meta_outputs;

  TF_DISALLOW_COPY_AND_ASSIGN(MainOpKernel);

 protected:
  int bx, by, bz, tx, ty, tz;
  std::vector<void*> args, p_args;
  std::vector<std::vector<int64>> output_shapes;
};

#if GOOGLE_CUDA
REGISTER_KERNEL_BUILDER(Name(OP_NAME).Device(DEVICE_GPU), MainOpKernel<GPUDevice>);
#endif
// REGISTER_KERNEL_BUILDER(Name(OP_NAME).Device(DEVICE_CPU), MainOpKernel<CPUDevice>);

}
}  // namespace tensorflow

