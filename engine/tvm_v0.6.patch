diff --git a/python/tvm/autotvm/tuner/tuner.py b/python/tvm/autotvm/tuner/tuner.py
index 76d088f4c..5dcf8e8d9 100644
--- a/python/tvm/autotvm/tuner/tuner.py
+++ b/python/tvm/autotvm/tuner/tuner.py
@@ -122,7 +122,7 @@ class Tuner(object):
             configs = self.next_batch(min(n_parallel, n_trial - i))
 
             inputs = [MeasureInput(self.task.target, self.task, config) for config in configs]
-            results = measure_batch(inputs)
+            results = self.measure_batch(inputs) if hasattr(self, 'measure_batch') else measure_batch(inputs)
 
             # keep best config
             for k, (inp, res) in enumerate(zip(inputs, results)):
diff --git a/src/codegen/codegen_c.cc b/src/codegen/codegen_c.cc
index eab542dd3..041f858be 100644
--- a/src/codegen/codegen_c.cc
+++ b/src/codegen/codegen_c.cc
@@ -155,7 +155,7 @@ std::string CodeGenC::GetBufferRef(
   }
   bool is_vol = volatile_buf_.count(buffer) != 0;
   if (t.lanes() == 1) {
-    if (!HandleTypeMatch(buffer, t) || is_vol) {
+    if (strcmp(getenv("BACKEND"), "c-hlsl") && (!HandleTypeMatch(buffer, t) || is_vol)) {
       os << "((";
       if (is_vol) {
         os << "volatile ";
@@ -808,6 +808,9 @@ void CodeGenC::VisitStmt_(const AttrStmt* op) {
     IterVar iv = Downcast<IterVar>(op->node);
     if (iv->thread_tag.length() != 0) {
       if (!var_idmap_.count(iv->var.get())) {
+        int nthread = static_cast<int>(op->value.as<IntImm>()->value);
+        if (iv->thread_tag.find("threadIdx.") == 0 || iv->thread_tag.find("blockIdx.") == 0)
+          this->stream << "  // [thread_extent] " << iv->thread_tag << " = " << nthread << "\n";
         BindThreadIndex(iv);
       }
     }
diff --git a/src/codegen/codegen_cuda.cc b/src/codegen/codegen_cuda.cc
index 6656fa077..83554dadd 100644
--- a/src/codegen/codegen_cuda.cc
+++ b/src/codegen/codegen_cuda.cc
@@ -28,6 +28,7 @@
 #include <string>
 #include "literal/cuda_half_t.h"
 #include "codegen_cuda.h"
+#include "datatype/registry.h"
 
 namespace tvm {
 namespace codegen {
@@ -49,6 +50,8 @@ void CodeGenCUDA::AddFunction(LoweredFunc f) {
 }
 
 std::string CodeGenCUDA::Finish() {
+  return CodeGenC::Finish();
+
   if (enable_fp16_) {
     decl_stream << "#if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ >= 530)\n";
     decl_stream << "#include <cuda_fp16.h>\n";
@@ -195,6 +198,9 @@ void CodeGenCUDA::PrintType(Type t, std::ostream& os) {  // NOLINT(*)
       os << lanes; return;
     }
   }
+  auto name = tvm::datatype::Registry::Global()->GetTypeName(t.code());
+  os << name; return;
+
   LOG(FATAL) << "Cannot convert type " << t << " to CUDA type";
 }
 
diff --git a/src/lang/expr_operator.cc b/src/lang/expr_operator.cc
index 220d4378c..cc435d138 100644
--- a/src/lang/expr_operator.cc
+++ b/src/lang/expr_operator.cc
@@ -208,11 +208,11 @@ Expr operator%(Expr a, Expr b) {
 
 // TODO(tqchen): switch to floordiv
 Expr indexdiv(Expr a, Expr b) {
-  return floordiv(a, b);
+  return truncdiv(a, b);
 }
 
 Expr indexmod(Expr a, Expr b) {
-  return floormod(a, b);
+  return truncmod(a, b);
 }
 
 Expr floordiv(Expr a, Expr b) {
diff --git a/src/pass/arg_binder.cc b/src/pass/arg_binder.cc
index f892b6b95..3b1f16ab9 100644
--- a/src/pass/arg_binder.cc
+++ b/src/pass/arg_binder.cc
@@ -178,7 +178,8 @@ void ArgBinder::BindDLTensor(const Buffer& buffer,
   Type dtype = buffer->dtype;
   std::ostringstream type_err_msg;
   type_err_msg << arg_name << ".dtype is expected to be " << dtype;
-  Expr cond = (TVMArrayGet(UInt(8), handle, intrinsic::kArrTypeCode) ==
+  Expr cond = UIntImm::make(UInt(8), dtype.code()) > UIntImm::make(UInt(8), kCustomBegin) ||
+               (TVMArrayGet(UInt(8), handle, intrinsic::kArrTypeCode) ==
                UIntImm::make(UInt(8), dtype.code()) &&
                TVMArrayGet(UInt(8), handle, intrinsic::kArrTypeBits) ==
                UIntImm::make(UInt(8), dtype.bits()) &&
diff --git a/src/pass/split_host_device.cc b/src/pass/split_host_device.cc
index 2239e5a07..b6edd7c09 100644
--- a/src/pass/split_host_device.cc
+++ b/src/pass/split_host_device.cc
@@ -95,6 +95,7 @@ class IRUseDefAnalysis : public IRMutator {
   }
 
   Stmt Mutate_(const Store *op, const Stmt& s) final {
+    this->output_hints.insert(op->buffer_var.get()->name_hint);
     this->HandleUse(op->buffer_var);
     return IRMutator::Mutate_(op, s);
   }
@@ -155,6 +156,7 @@ class IRUseDefAnalysis : public IRMutator {
   // The fields are publically readible to
   // be accessible to the users.
   bool visit_thread_extent_{true};
+  std::unordered_set<std::string> output_hints;
   Array<Var> undefined_;
   Array<IterVar> thread_axis_;
   Array<Expr> thread_extent_;
@@ -208,7 +210,15 @@ class HostDeviceSplitter : public IRMutator {
     n->func_type = kDeviceFunc;
     n->thread_axis = m.thread_axis_;
     // Strictly order the arguments: Var pointers, positional arguments.
-    for (Var v : m.undefined_) {
+    std::vector<Var> ordered_args(m.undefined_.begin(), m.undefined_.end());
+    std::sort(ordered_args.begin(), ordered_args.end(), [&](const Var &x, const Var &y) {
+      int x_access = m.output_hints.count(x.get()->name_hint);
+      int y_access = m.output_hints.count(y.get()->name_hint);
+      if (x_access != y_access)
+        return x_access < y_access;
+      return x.get()->name_hint < y.get()->name_hint;
+    });
+    for (Var v : ordered_args) {
       if (v.type().is_handle()) {
         n->args.push_back(v);
         // mark handle data type.
